---
phase: 03-plugin-restructuring
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - pkg/plugin/options_test.go
  - internal/reporter/report.go
autonomous: true

must_haves:
  truths:
    - "Plugin tests use FakeClient and bytes.Buffer — no real cluster needed"
    - "Complete method tested: defaults populated, kubernetes client created when nil, skipped when pre-injected"
    - "Validate method tested: rejects invalid output format, rejects topImages < 1, accepts valid configs"
    - "Run method tested end-to-end: creates analysis from fake pods/nodes, generates report to buffer"
    - "Run with JSON output tested: valid JSON written to stdout"
    - "Run with namespace filter tested: only matching pods analyzed"
    - "Run with no namespace tested: all node images analyzed"
    - "Test coverage exceeds 70% across all packages combined"
  artifacts:
    - path: "pkg/plugin/options_test.go"
      provides: "Comprehensive tests for AnalyzeOptions Complete/Validate/Run"
      contains: "TestAnalyzeOptions_Complete"
    - path: "internal/reporter/report.go"
      provides: "Reporter accepting io.Writer for testability"
      contains: "io.Writer"
  key_links:
    - from: "pkg/plugin/options_test.go"
      to: "pkg/kubernetes/fake.go"
      via: "Tests inject FakeClient into AnalyzeOptions"
      pattern: "kubernetes\\.NewFakeClient"
    - from: "pkg/plugin/options_test.go"
      to: "pkg/plugin/options.go"
      via: "Tests exercise Complete/Validate/Run methods"
      pattern: "AnalyzeOptions"
    - from: "pkg/plugin/options.go"
      to: "internal/reporter/report.go"
      via: "Run passes io.Writer to GenerateReport"
      pattern: "GenerateReportTo"
---

<objective>
Add comprehensive plugin tests and make Reporter output testable, achieving >70% total project coverage.

Purpose: The plugin options struct created in Plan 01 needs thorough testing to validate the Complete/Validate/Run pattern works correctly with injected dependencies. The Reporter needs a minor refactor to accept `io.Writer` so the plugin can direct report output to the injected writer (enabling test assertions on output). This brings total project coverage above 70%.

Output: `pkg/plugin/options_test.go` with comprehensive tests, updated `internal/reporter/report.go` with writer-accepting method, and total project test coverage >70%.
</objective>

<execution_context>
@/Users/rnathani/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rnathani/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-plugin-restructuring/03-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Make Reporter output testable by accepting io.Writer</name>
  <files>internal/reporter/report.go, pkg/plugin/options.go</files>
  <action>
The current `Reporter.GenerateReport` hardcodes `os.Stdout` when calling `printer.Print(os.Stdout, analysis)`. This means plugin tests cannot capture report output. Add a `GenerateReportTo` method that accepts `io.Writer`.

**Update `internal/reporter/report.go`:**

Add a new method `GenerateReportTo` that accepts an `io.Writer`:

```go
// GenerateReportTo generates a report to the specified writer
func (r *Reporter) GenerateReportTo(w io.Writer, analysis *types.ImageAnalysis) error {
    var printer types.Printer
    switch r.outputFormat {
    case "table":
        printer = NewTablePrinter(r.showHistogram, r.noColor, r.topImages)
    case "json":
        printer = NewJSONPrinter()
    default:
        return fmt.Errorf("unsupported output format: %s", r.outputFormat)
    }
    return printer.Print(w, analysis)
}
```

Add the `io` import if not already present.

Keep the existing `GenerateReport` method for backward compatibility, but refactor it to delegate:

```go
// GenerateReport generates a report to os.Stdout
func (r *Reporter) GenerateReport(analysis *types.ImageAnalysis) error {
    return r.GenerateReportTo(os.Stdout, analysis)
}
```

**Update `pkg/plugin/options.go`:**

Change the Run method to use `GenerateReportTo(o.Out, analysis)` instead of `GenerateReport(analysis)`:

```go
// In the Run method, change the report generation section:
rep := reporter.NewReporter(o.OutputFormat)
rep.SetNoColor(o.NoColor)
rep.SetTopImages(o.TopImages)
if err := rep.GenerateReportTo(o.Out, analysis); err != nil {
    return fmt.Errorf("failed to generate report: %w", err)
}
```

This means when tests set `o.Out = &bytes.Buffer{}`, report output goes to the buffer instead of stdout.

**IMPORTANT:** The existing `GenerateReport` method still works for any code calling it directly. This is a backward-compatible change.
  </action>
  <verify>Run `go build ./...` — must compile. Run `go test ./...` — all existing tests pass. Verify `GenerateReportTo` method exists: `grep 'GenerateReportTo' internal/reporter/report.go`. Verify Run uses `GenerateReportTo`: `grep 'GenerateReportTo' pkg/plugin/options.go`.</verify>
  <done>Reporter has GenerateReportTo(io.Writer, *ImageAnalysis) method. GenerateReport delegates to GenerateReportTo(os.Stdout, ...). Plugin Run method uses GenerateReportTo(o.Out, ...) so test output is capturable. All existing tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Add comprehensive plugin tests</name>
  <files>pkg/plugin/options_test.go</files>
  <action>
Create `pkg/plugin/options_test.go` with thorough coverage of all three methods.

**Test helpers at the top:**

```go
package plugin

import (
    "bytes"
    "context"
    "encoding/json"
    "strings"
    "testing"

    "github.com/ronaknnathani/kubectl-analyze-images/pkg/kubernetes"
    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/require"
    corev1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// Helper to create test pods
func testPod(name, namespace string, images ...string) *corev1.Pod {
    containers := make([]corev1.Container, len(images))
    for i, img := range images {
        containers[i] = corev1.Container{Name: fmt.Sprintf("c%d", i), Image: img}
    }
    return &corev1.Pod{
        ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: namespace},
        Spec:       corev1.PodSpec{Containers: containers},
    }
}

// Helper to create test nodes with images
func testNode(name string, images map[string]int64) *corev1.Node {
    nodeImages := make([]corev1.ContainerImage, 0, len(images))
    for img, size := range images {
        nodeImages = append(nodeImages, corev1.ContainerImage{
            Names: []string{img}, SizeBytes: size,
        })
    }
    return &corev1.Node{
        ObjectMeta: metav1.ObjectMeta{Name: name},
        Status:     corev1.NodeStatus{Images: nodeImages},
    }
}
```

**Test 1: TestAnalyzeOptions_Complete**

Table-driven test cases:
- "defaults populated": Create empty AnalyzeOptions, call Complete, verify OutputFormat="table", TopImages=25, Out and ErrOut are non-nil
- "preserves explicit values": Set OutputFormat="json", TopImages=10, call Complete, verify values unchanged
- "skips kubernetes client when pre-injected": Pre-set KubernetesClient to FakeClient, call Complete, verify it remains the same FakeClient instance (not replaced)
- "creates kubernetes client when nil": DO NOT test this case directly (requires real kubeconfig). Instead, verify that KubernetesClient is still nil after Complete when no kubeconfig exists — this will return an error, confirming it tried to create one

```go
func TestAnalyzeOptions_Complete(t *testing.T) {
    t.Run("defaults populated", func(t *testing.T) {
        o := &AnalyzeOptions{
            KubernetesClient: kubernetes.NewFakeClient(), // pre-inject to avoid kubeconfig requirement
        }
        err := o.Complete()
        require.NoError(t, err)
        assert.Equal(t, "table", o.OutputFormat)
        assert.Equal(t, 25, o.TopImages)
        assert.NotNil(t, o.Out)
        assert.NotNil(t, o.ErrOut)
    })

    t.Run("preserves explicit values", func(t *testing.T) {
        buf := &bytes.Buffer{}
        o := &AnalyzeOptions{
            OutputFormat:     "json",
            TopImages:        10,
            Out:              buf,
            KubernetesClient: kubernetes.NewFakeClient(),
        }
        err := o.Complete()
        require.NoError(t, err)
        assert.Equal(t, "json", o.OutputFormat)
        assert.Equal(t, 10, o.TopImages)
        assert.Equal(t, buf, o.Out) // same buffer instance
    })

    t.Run("skips kubernetes client when pre-injected", func(t *testing.T) {
        fakeClient := kubernetes.NewFakeClient()
        o := &AnalyzeOptions{
            KubernetesClient: fakeClient,
        }
        err := o.Complete()
        require.NoError(t, err)
        assert.Equal(t, fakeClient, o.KubernetesClient) // same instance
    })
}
```

**Test 2: TestAnalyzeOptions_Validate**

Table-driven test cases:
- "valid table format": OutputFormat="table", TopImages=25 -> no error
- "valid json format": OutputFormat="json", TopImages=10 -> no error
- "invalid output format": OutputFormat="yaml" -> error containing "invalid output format"
- "empty output format after Complete": OutputFormat="" but call Complete first -> no error (defaults to "table")
- "topImages zero": TopImages=0 -> error containing "must be at least 1"
- "topImages negative": TopImages=-5 -> error containing "must be at least 1"
- "topImages one": TopImages=1 -> no error

```go
func TestAnalyzeOptions_Validate(t *testing.T) {
    tests := []struct {
        name        string
        opts        AnalyzeOptions
        expectError string
    }{
        {name: "valid table format", opts: AnalyzeOptions{OutputFormat: "table", TopImages: 25}},
        {name: "valid json format", opts: AnalyzeOptions{OutputFormat: "json", TopImages: 10}},
        {name: "invalid output format", opts: AnalyzeOptions{OutputFormat: "yaml", TopImages: 25}, expectError: "invalid output format"},
        {name: "topImages zero", opts: AnalyzeOptions{OutputFormat: "table", TopImages: 0}, expectError: "must be at least 1"},
        {name: "topImages negative", opts: AnalyzeOptions{OutputFormat: "table", TopImages: -5}, expectError: "must be at least 1"},
        {name: "topImages one is valid", opts: AnalyzeOptions{OutputFormat: "table", TopImages: 1}},
    }

    for _, tc := range tests {
        t.Run(tc.name, func(t *testing.T) {
            err := tc.opts.Validate()
            if tc.expectError != "" {
                require.Error(t, err)
                assert.Contains(t, err.Error(), tc.expectError)
            } else {
                require.NoError(t, err)
            }
        })
    }
}
```

**Test 3: TestAnalyzeOptions_Run_TableOutput**

End-to-end test with table output:
- Create FakeClient with 2 pods in "default" namespace (nginx:1.21, redis:6.2)
- Create FakeClient with 1 node containing both images with known sizes
- Set Out and ErrOut to bytes.Buffer
- Set OutputFormat="table", TopImages=25
- Call Run, verify no error
- Verify the output buffer contains expected strings: "Image Analysis Summary", "nginx:1.21", "redis:6.2"

```go
func TestAnalyzeOptions_Run_TableOutput(t *testing.T) {
    pod1 := testPod("pod1", "default", "nginx:1.21")
    pod2 := testPod("pod2", "default", "redis:6.2")
    node := testNode("node1", map[string]int64{
        "nginx:1.21": 100000000, // 100MB
        "redis:6.2":  50000000,  // 50MB
    })

    out := &bytes.Buffer{}
    errOut := &bytes.Buffer{}

    o := &AnalyzeOptions{
        Namespace:        "default",
        OutputFormat:     "table",
        TopImages:        25,
        ShowHistogram:    true,
        KubernetesClient: kubernetes.NewFakeClient(pod1, pod2, node),
        Out:              out,
        ErrOut:           errOut,
    }

    err := o.Run(context.Background())
    require.NoError(t, err)

    output := out.String()
    assert.Contains(t, output, "Analyzing images in namespace: default")
    assert.Contains(t, output, "Image Analysis Summary")
    assert.Contains(t, output, "nginx:1.21")
    assert.Contains(t, output, "redis:6.2")
}
```

**Test 4: TestAnalyzeOptions_Run_JSONOutput**

End-to-end test with JSON output:
- Same pod/node setup as table test
- Set OutputFormat="json"
- Call Run, verify no error
- Parse the output as JSON (after stripping the "Analyzing images..." prefix lines)
- Verify JSON has `images` array with 2 entries, `summary.totalImages` is 2

```go
func TestAnalyzeOptions_Run_JSONOutput(t *testing.T) {
    pod1 := testPod("pod1", "default", "nginx:1.21")
    node := testNode("node1", map[string]int64{
        "nginx:1.21": 100000000,
    })

    out := &bytes.Buffer{}

    o := &AnalyzeOptions{
        Namespace:        "default",
        OutputFormat:     "json",
        TopImages:        25,
        KubernetesClient: kubernetes.NewFakeClient(pod1, node),
        Out:              out,
        ErrOut:           &bytes.Buffer{},
    }

    err := o.Run(context.Background())
    require.NoError(t, err)

    // The output contains "Analyzing images..." header lines followed by JSON
    // Find the JSON portion (starts with '{')
    output := out.String()
    jsonStart := strings.Index(output, "{")
    require.True(t, jsonStart >= 0, "expected JSON output, got: %s", output)

    var result map[string]interface{}
    err = json.Unmarshal([]byte(output[jsonStart:]), &result)
    require.NoError(t, err, "failed to parse JSON output")

    // Verify structure
    summary, ok := result["summary"].(map[string]interface{})
    require.True(t, ok, "expected summary object in JSON")
    assert.Equal(t, float64(1), summary["totalImages"])
}
```

**Test 5: TestAnalyzeOptions_Run_AllNamespaces**

Test with empty namespace (all namespaces mode):
- Create FakeClient with 1 node containing 3 images (no pods needed for all-namespaces)
- Set Namespace="" (empty)
- Call Run, verify no error
- Verify output contains "Analyzing images in namespace: All"
- Verify output contains all 3 image names

**Test 6: TestAnalyzeOptions_Run_WithLabelSelector**

Test with label selector:
- Create FakeClient with pods, one matching label and one not
- Set LabelSelector to match only one pod
- Call Run, verify output contains "Using label selector:"

After all tests, run coverage and verify:
- `go test -cover ./pkg/plugin/` — should show high coverage (>80%)
- `go test -cover ./...` — verify total project coverage >70%
  </action>
  <verify>Run `go test ./pkg/plugin/ -v` — all tests pass. Run `go test -cover ./pkg/plugin/` — coverage >80%. Run `go test -cover ./...` — verify all packages. Run `go test ./...` — all project tests pass (no regressions). Run `make build` — build succeeds.</verify>
  <done>pkg/plugin/options_test.go has tests for Complete (3 cases), Validate (6 cases), Run with table output, Run with JSON output, Run with all-namespaces, Run with label selector. Plugin package coverage >80%. Total project test coverage >70%. All project tests pass. Build succeeds.</done>
</task>

</tasks>

<verification>
1. `go build ./...` compiles the entire project without errors
2. `go vet ./...` passes
3. `go test ./...` passes — all tests (Phase 1 + Phase 2 + Phase 3) green
4. `go test -cover ./pkg/plugin/` shows >80% coverage
5. `go test -cover ./...` shows total project coverage >70% (currently at ~45.8%, adding plugin tests should push well above 70%)
6. `make build` succeeds
7. `GenerateReportTo` method exists in reporter and accepts io.Writer
8. Plugin Run method uses `GenerateReportTo(o.Out, ...)` for testable output
9. JSON output test parses valid JSON from the output buffer
10. Table output test verifies expected strings in the output buffer
</verification>

<success_criteria>
- Plugin tests cover Complete, Validate, and Run with multiple scenarios
- Tests use FakeClient and bytes.Buffer (no real cluster needed)
- Reporter accepts io.Writer via GenerateReportTo (backward compatible)
- Plugin Run directs report output through injected writer
- Total project test coverage >70% across all packages
- All existing tests still pass (no regressions)
- `make build` produces working binary
</success_criteria>

<output>
After completion, create `.planning/phases/03-plugin-restructuring/03-02-SUMMARY.md`
</output>
